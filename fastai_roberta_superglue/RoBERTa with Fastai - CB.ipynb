{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Using RoBERTa with Fastai - CB Tutorial "]}, {"cell_type": "markdown", "metadata": {}, "source": ["This notebook follows the tutorial @ https://medium.com/@devkosal/superglue-roberta-with-fastai-for-rte-task-c362961be957"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"ExecuteTime": {"end_time": "2019-09-04T02:30:58.738741Z", "start_time": "2019-09-04T02:30:57.798726Z"}}, "outputs": [], "source": ["from fastai.text import *\n", "from fastai.metrics import *\n", "from transformers import RobertaTokenizer"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2019-09-04T02:30:58.744883Z", "start_time": "2019-09-04T02:30:58.740327Z"}}, "outputs": [], "source": ["# Creating a config object to store task specific information\n", "class Config(dict):\n", "    def __init__(self, **kwargs):\n", "        super().__init__(**kwargs)\n", "        for k, v in kwargs.items():\n", "            setattr(self, k, v)\n", "    \n", "    def set(self, key, val):\n", "        self[key] = val\n", "        setattr(self, key, val)\n", "        \n", "config = Config(\n", "    task = \"CB\",\n", "    testing=False,\n", "    seed = 2019,\n", "    roberta_model_name='roberta-base', # can also be exchanged with roberta-large \n", "    max_lr=1e-5,\n", "    epochs=10,\n", "    use_fp16=False,\n", "    bs=4, \n", "    max_seq_len=256, \n", "    num_labels = 3,\n", "    hidden_dropout_prob=.05,\n", "    hidden_size=768, # 1024 for roberta-large\n", "    start_tok = \"<s>\",\n", "    end_tok = \"</s>\",\n", "    mark_fields=True,\n", ")"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2019-09-04T02:30:58.756712Z", "start_time": "2019-09-04T02:30:58.746261Z"}}, "outputs": [], "source": ["path = Path(\".\")\n", "data_path = path/\"data\""]}, {"cell_type": "code", "execution_count": 6, "metadata": {"ExecuteTime": {"end_time": "2019-09-04T02:31:00.072931Z", "start_time": "2019-09-04T02:31:00.050079Z"}}, "outputs": [], "source": ["train = pd.read_json(data_path/config.task/\"train.jsonl\",lines=True)\n", "val = pd.read_json(data_path/config.task/\"val.jsonl\",lines=True)\n", "test = pd.read_json(data_path/config.task/\"test.jsonl\",lines=True)"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.053988Z", "start_time": "2019-09-03T23:52:36.049310Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(56, 3)\n"]}], "source": ["# drop the unnecessary idx column\n", "for df in (train,val):\n", "    if \"idx\" in df.columns: df.drop(\"idx\",axis=1,inplace=True)\n", "        \n", "if config.testing:\n", "    train = train[:100]\n", "    val = val[:100]\n", "    \n", "print(df.shape)"]}, {"cell_type": "code", "execution_count": 28, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.067099Z", "start_time": "2019-09-03T23:52:36.055142Z"}}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>hypothesis</th>\n", "      <th>label</th>\n", "      <th>premise</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>the language was peeled down</td>\n", "      <td>entailment</td>\n", "      <td>It was a complex language. Not written down bu...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>no women are allowed to take part in this ritual</td>\n", "      <td>entailment</td>\n", "      <td>It is part of their religion, a religion I do ...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Gustave was shepherded into creative retreat a...</td>\n", "      <td>entailment</td>\n", "      <td>The Paris to Rouen railway was being extended ...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>Gustave was driven to creative retreat in Croi...</td>\n", "      <td>entailment</td>\n", "      <td>Part of it was to be compulsorily purchased. Y...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>buying places is a hobby</td>\n", "      <td>entailment</td>\n", "      <td>Some of them, like for instance the farm in Co...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                                          hypothesis       label  \\\n", "0                       the language was peeled down  entailment   \n", "1   no women are allowed to take part in this ritual  entailment   \n", "2  Gustave was shepherded into creative retreat a...  entailment   \n", "3  Gustave was driven to creative retreat in Croi...  entailment   \n", "4                           buying places is a hobby  entailment   \n", "\n", "                                             premise  \n", "0  It was a complex language. Not written down bu...  \n", "1  It is part of their religion, a religion I do ...  \n", "2  The Paris to Rouen railway was being extended ...  \n", "3  Part of it was to be compulsorily purchased. Y...  \n", "4  Some of them, like for instance the farm in Co...  "]}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": ["train.head()"]}, {"cell_type": "code", "execution_count": 29, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.077736Z", "start_time": "2019-09-03T23:52:36.068230Z"}, "scrolled": true}, "outputs": [{"data": {"text/plain": ["contradiction    119\n", "entailment       115\n", "neutral           16\n", "Name: label, dtype: int64"]}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": ["train.label.value_counts()"]}, {"cell_type": "code", "execution_count": 30, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.086245Z", "start_time": "2019-09-03T23:52:36.079426Z"}}, "outputs": [], "source": ["feat_cols = [\"premise\",\"hypothesis\"]\n", "label_cols = \"label\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setting Up the Tokenizer"]}, {"cell_type": "code", "execution_count": 31, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.093982Z", "start_time": "2019-09-03T23:52:36.087757Z"}}, "outputs": [], "source": ["class FastAiRobertaTokenizer(BaseTokenizer):\n", "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n", "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n", "        self._pretrained_tokenizer = tokenizer\n", "        self.max_seq_len = max_seq_len \n", "    def __call__(self, *args, **kwargs): \n", "        return self \n", "    def tokenizer(self, t:str) -> List[str]: \n", "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n", "        if config.mark_fields:\n", "            sub = 2 # subtraction in totoal seq_length to be made due to adding spcl tokens\n", "            assert \"xxfld\" in t\n", "            t = t.replace(\"xxfld 1\",\"\") # remove the xxfld 1 special token from fastai\n", "            # converting fastai field sep token to Roberta\n", "            t = re.split(r'xxfld \\d+', t) \n", "            res = []\n", "            for i in range(len(t)-1): # loop over the number of additional fields and the Roberta sep\n", "                res += self._pretrained_tokenizer.tokenize(t[i]) + [config.end_tok, config.end_tok]\n", "                sub += 2 # increase our subtractions since we added more spcl tokens\n", "            res += self._pretrained_tokenizer.tokenize(t[-1]) # add the last sequence\n", "            return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok] \n", "        \n", "        res = self._pretrained_tokenizer.tokenize(t)\n", "        return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok]"]}, {"cell_type": "code", "execution_count": 32, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.427223Z", "start_time": "2019-09-03T23:52:36.095149Z"}}, "outputs": [], "source": ["# create fastai tokenizer for roberta\n", "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n", "\n", "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n", "                             pre_rules=[], post_rules=[])"]}, {"cell_type": "code", "execution_count": 33, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.549924Z", "start_time": "2019-09-03T23:52:36.428608Z"}}, "outputs": [], "source": ["# create fastai vocabulary for roberta\n", "roberta_tok.save_vocabulary(path)\n", "\n", "with open('vocab.json', 'r') as f:\n", "    roberta_vocab_dict = json.load(f)\n", "    \n", "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))"]}, {"cell_type": "code", "execution_count": 34, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.555292Z", "start_time": "2019-09-03T23:52:36.551292Z"}}, "outputs": [], "source": ["# Setting up pre-processors\n", "class RobertaTokenizeProcessor(TokenizeProcessor):\n", "    def __init__(self, tokenizer):\n", "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False, mark_fields=config.mark_fields)\n", "\n", "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n", "    def __init__(self, *args, **kwargs):\n", "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n", "\n", "\n", "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n", "    \"\"\"\n", "    Constructing preprocessors for Roberta\n", "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n", "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n", "    \"\"\"\n", "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setting up the DataBunch"]}, {"cell_type": "code", "execution_count": 35, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.563667Z", "start_time": "2019-09-03T23:52:36.556702Z"}}, "outputs": [], "source": ["# Creating a Roberta specific DataBunch class\n", "class RobertaDataBunch(TextDataBunch):\n", "    \"Create a `TextDataBunch` suitable for training Roberta\"\n", "    @classmethod\n", "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n", "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n", "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n", "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n", "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n", "        val_bs = ifnone(val_bs, bs)\n", "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n", "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n", "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n", "        dataloaders = [train_dl]\n", "        for ds in datasets[1:]:\n", "            lengths = [len(t) for t in ds.x.items]\n", "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n", "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n", "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"]}, {"cell_type": "code", "execution_count": 36, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:36.570550Z", "start_time": "2019-09-03T23:52:36.565011Z"}}, "outputs": [], "source": ["class RobertaTextList(TextList):\n", "    _bunch = RobertaDataBunch\n", "    _label_cls = TextList"]}, {"cell_type": "code", "execution_count": 37, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:38.226583Z", "start_time": "2019-09-03T23:52:36.571900Z"}}, "outputs": [], "source": ["# loading the tokenizer and vocab processors\n", "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n", "\n", "# creating our databunch \n", "data = ItemLists(\".\", RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor),\n", "                      RobertaTextList.from_df(val, \".\", cols=feat_cols, processor=processor)\n", "                ) \\\n", "       .label_from_df(cols=label_cols, label_cls=CategoryList) \\\n", "       .add_test(RobertaTextList.from_df(test, \".\", cols=feat_cols, processor=processor)) \\\n", "       .databunch(bs=config.bs,pad_first=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Building the Model"]}, {"cell_type": "code", "execution_count": 38, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:38.232898Z", "start_time": "2019-09-03T23:52:38.228130Z"}}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "from transformers import RobertaForSequenceClassification\n", "\n", "# defining our model architecture \n", "class RobertaForSequenceClassificationModel(nn.Module):\n", "    def __init__(self,num_labels=config.num_labels):\n", "        super(RobertaForSequenceClassificationModel,self).__init__()\n", "        self.num_labels = num_labels\n", "        self.roberta = RobertaForSequenceClassification.from_pretrained(config.roberta_model_name,num_labels= self.num_labels)\n", "\n", "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n", "        outputs = self.roberta(input_ids, token_type_ids, attention_mask)\n", "        logits = outputs[0] \n", "        return logits"]}, {"cell_type": "code", "execution_count": 39, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:52:41.690696Z", "start_time": "2019-09-03T23:52:38.234218Z"}}, "outputs": [], "source": ["roberta_model = RobertaForSequenceClassificationModel() \n", "\n", "learn = Learner(data, roberta_model, metrics=[accuracy])"]}, {"cell_type": "code", "execution_count": 40, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:54:57.085222Z", "start_time": "2019-09-03T23:52:41.692566Z"}}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: left;\">\n", "      <th>epoch</th>\n", "      <th>train_loss</th>\n", "      <th>valid_loss</th>\n", "      <th>accuracy</th>\n", "      <th>time</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>0</td>\n", "      <td>1.079415</td>\n", "      <td>1.069071</td>\n", "      <td>0.428571</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>0.960143</td>\n", "      <td>0.842565</td>\n", "      <td>0.678571</td>\n", "      <td>00:12</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>0.768365</td>\n", "      <td>0.631433</td>\n", "      <td>0.750000</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>0.477261</td>\n", "      <td>0.324045</td>\n", "      <td>0.892857</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>0.279063</td>\n", "      <td>0.182191</td>\n", "      <td>0.928571</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>0.151150</td>\n", "      <td>0.165908</td>\n", "      <td>0.964286</td>\n", "      <td>00:14</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>0.079247</td>\n", "      <td>0.116142</td>\n", "      <td>0.964286</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>0.043399</td>\n", "      <td>0.111627</td>\n", "      <td>0.964286</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>0.027586</td>\n", "      <td>0.103498</td>\n", "      <td>0.964286</td>\n", "      <td>00:12</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>0.020711</td>\n", "      <td>0.102435</td>\n", "      <td>0.964286</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n", "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Getting Predictions"]}, {"cell_type": "code", "execution_count": 41, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:54:57.090526Z", "start_time": "2019-09-03T23:54:57.086949Z"}}, "outputs": [], "source": ["def get_preds_as_nparray(ds_type) -> np.ndarray:\n", "    learn.model.roberta.eval()\n", "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n", "    sampler = [i for i in data.dl(ds_type).sampler]\n", "    reverse_sampler = np.argsort(sampler)\n", "    ordered_preds = preds[reverse_sampler, :]\n", "    pred_values = np.argmax(ordered_preds, axis=1)\n", "    return ordered_preds, pred_values"]}, {"cell_type": "code", "execution_count": 42, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:54:57.934272Z", "start_time": "2019-09-03T23:54:57.091801Z"}}, "outputs": [], "source": ["# val preds\n", "preds, pred_values = get_preds_as_nparray(DatasetType.Valid)"]}, {"cell_type": "code", "execution_count": 43, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:54:57.939243Z", "start_time": "2019-09-03T23:54:57.935988Z"}, "scrolled": true}, "outputs": [{"data": {"text/plain": ["0.9642857142857143"]}, "execution_count": 43, "metadata": {}, "output_type": "execute_result"}], "source": ["# accuracy for valid valid\n", "(pred_values == data.valid_ds.y.items).mean()"]}, {"cell_type": "code", "execution_count": 44, "metadata": {"ExecuteTime": {"end_time": "2019-09-03T23:55:00.674531Z", "start_time": "2019-09-03T23:54:57.940330Z"}}, "outputs": [], "source": ["# test preds\n", "_, test_pred_values = get_preds_as_nparray(DatasetType.Test)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}