{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts SQuAD json files into pandas ready csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:15:10.684729Z",
     "start_time": "2020-01-21T16:15:10.678742Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = \"albert-base-v2\"\n",
    "squad_version = \"1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:15:12.363704Z",
     "start_time": "2020-01-21T16:15:11.701001Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import sentencepiece\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:15:15.116102Z",
     "start_time": "2020-01-21T16:15:14.872640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert\n"
     ]
    }
   ],
   "source": [
    "model_name = re.findall(r\"(.+?)-\",pretrained_model)[0]; print(model_name)\n",
    "tok = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "data_path = Path(f\"../../data/SQuAD/{squad_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:56:55.351953Z",
     "start_time": "2020-01-21T17:56:03.728372Z"
    }
   },
   "outputs": [],
   "source": [
    "def squad_parser(directory: Path, tok , data_set: str=\"train\"):\n",
    "    \"\"\"\n",
    "    convert squad train and dev jsons to dfs\n",
    "    \"\"\"\n",
    "    ds_dir = data_set + f\"-v{squad_version}.json\"\n",
    "    with open(directory/ds_dir) as f:\n",
    "        file = json.load(f)\n",
    "    ques, paras, answers, idxs, seq_len = [],[],[],[],[]\n",
    "    for item in file[\"data\"]:\n",
    "        for paragraphs in item[\"paragraphs\"]:\n",
    "            context = paragraphs[\"context\"]\n",
    "            tok_context = tok.tokenize(context)\n",
    "            for qas in paragraphs[\"qas\"]:\n",
    "                for answer in qas[\"answers\"]:\n",
    "                    start_idx = len(tok.tokenize(context[:answer[\"answer_start\"]]))\n",
    "                    end_idx = start_idx + len(tok.tokenize(answer[\"text\"]))\n",
    "                    ques.append(qas[\"question\"])\n",
    "                    paras.append(context)\n",
    "                    answers.append(tok_context[start_idx:end_idx])\n",
    "                    idxs.append([start_idx,end_idx])\n",
    "                    seq_len.append(len(tok_context + tok.tokenize(qas[\"question\"])))\n",
    "    return pd.DataFrame({\"question\":ques,\"paragraph\" : paras, \"answer\":answers, \"idxs\":idxs, \"seq_len\":seq_len})\n",
    "\n",
    "if not os.path.exists(data_path/f\"train_{model_name}.csv\"):\n",
    "    train = squad_parser(data_path, tok, \"train\")\n",
    "    val = squad_parser(data_path, tok,\"dev\")\n",
    "\n",
    "    # export to csv\n",
    "    train.to_csv(data_path/f\"train_{model_name}.csv\",index=False)\n",
    "    val.to_csv(data_path/f\"val_{model_name}.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
